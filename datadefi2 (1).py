# -*- coding: utf-8 -*-
"""datadefi2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MW6SamHSdVdkR4cQTEvPwBPe_ZVdSUPN
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import tree
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.ensemble import RandomForestClassifier
from matplotlib import pyplot
from sklearn.decomposition import PCA
from dask.distributed import Client
import joblib
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_validate, KFold
from sklearn.svm import LinearSVC, SVC
from sklearn.pipeline import Pipeline
from sklearn.ensemble import VotingClassifier
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE

import os
import os.path

from pickle import TRUE
df = pd.read_csv('../content/dataDefi2.csv', sep=';')

df

## v0.1 Remove id
delist = ['encounter_id', 'patient_id', 'hospital_id', 'icu_id', 'apache_3j_bodysystem', 'apache_2_bodysystem']
df.drop(delist, axis=1, inplace=True)

df

## v0.2 Replace ',' with '.'
#print(df.columns)
repvir = ['bmi', 'height', 'pre_icu_los_days', 'weight', 'apache_3j_diagnosis', 'temp_apache', 'd1_temp_max', 'd1_temp_min', 'd1_potassium_max', 'd1_potassium_min', 'apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob']
for i in repvir:
    # if str(df[i]).find(',') != -1:
    df[i] = df[i].str.replace(',', '.').astype('float')

df

## v0.3 Clean and standardlize
dummcol = ['ethnicity', 'icu_admit_source', 'icu_stay_type', 'icu_type']
df = pd.get_dummies(data=df, columns=dummcol)
df['gender'] = df['gender'].apply(lambda x: 0 if x=='F' else 1)
# for j in df.columns.tolist():
#     df[j].replace(to_replace=[None],value=np.nan,inplace=True)
# df = df.groupby(df.columns, axis = 1).transform(lambda x: x.fillna(x.mean()))
for j in df.columns.tolist():
    df[j] = pd.to_numeric(df[j], errors='coerce')
for column in list(df.columns[df.isnull().sum() > 0]):
       mean_val = df[column].mean()
       df[column].fillna(mean_val, inplace=True)

print(df.columns)

df

std = StandardScaler()
std = std.fit_transform(df.loc[:, df.columns != 'hospital_death'])
attricol = ['age', 'bmi', 'elective_surgery', 'gender', 'height',
       'pre_icu_los_days', 'weight', 'apache_2_diagnosis',
       'apache_3j_diagnosis', 'apache_post_operative', 'arf_apache',
       'gcs_eyes_apache', 'gcs_motor_apache', 'gcs_unable_apache',
       'gcs_verbal_apache', 'heart_rate_apache', 'intubated_apache',
       'map_apache', 'resprate_apache', 'temp_apache', 'ventilated_apache',
       'd1_diasbp_max', 'd1_diasbp_min', 'd1_diasbp_noninvasive_max',
       'd1_diasbp_noninvasive_min', 'd1_heartrate_max', 'd1_heartrate_min',
       'd1_mbp_max', 'd1_mbp_min', 'd1_mbp_noninvasive_max',
       'd1_mbp_noninvasive_min', 'd1_resprate_max', 'd1_resprate_min',
       'd1_spo2_max', 'd1_spo2_min', 'd1_sysbp_max', 'd1_sysbp_min',
       'd1_sysbp_noninvasive_max', 'd1_sysbp_noninvasive_min', 'd1_temp_max',
       'd1_temp_min', 'h1_diasbp_max', 'h1_diasbp_min',
       'h1_diasbp_noninvasive_max', 'h1_diasbp_noninvasive_min',
       'h1_heartrate_max', 'h1_heartrate_min', 'h1_mbp_max', 'h1_mbp_min',
       'h1_mbp_noninvasive_max', 'h1_mbp_noninvasive_min', 'h1_resprate_max',
       'h1_resprate_min', 'h1_spo2_max', 'h1_spo2_min', 'h1_sysbp_max',
       'h1_sysbp_min', 'h1_sysbp_noninvasive_max', 'h1_sysbp_noninvasive_min',
       'd1_glucose_max', 'd1_glucose_min', 'd1_potassium_max',
       'd1_potassium_min', 'apache_4a_hospital_death_prob',
       'apache_4a_icu_death_prob', 'aids', 'cirrhosis', 'diabetes_mellitus',
       'hepatic_failure', 'immunosuppression', 'leukemia', 'lymphoma',
       'solid_tumor_with_metastasis',
       'ethnicity_African American', 'ethnicity_Asian', 'ethnicity_Caucasian',
       'ethnicity_Hispanic', 'ethnicity_Native American',
       'ethnicity_Other/Unknown', 'icu_admit_source_Accident & Emergency',
       'icu_admit_source_Floor', 'icu_admit_source_Operating Room / Recovery',
       'icu_admit_source_Other Hospital', 'icu_admit_source_Other ICU',
       'icu_stay_type_admit', 'icu_stay_type_readmit',
       'icu_stay_type_transfer', 'icu_type_CCU-CTICU', 'icu_type_CSICU',
       'icu_type_CTICU', 'icu_type_Cardiac ICU', 'icu_type_MICU',
       'icu_type_Med-Surg ICU', 'icu_type_Neuro ICU', 'icu_type_SICU']

std = pd.DataFrame(data=std, columns= attricol)
atop = pd.concat([std, pd.DataFrame(columns=['hospital_death'])])
atop['hospital_death'] = df['hospital_death']

std

## v0.4 show attributs' importance
atopdata, atoptarget = atop.loc[:, atop.columns != 'hospital_death'], atop.loc[:, atop.columns == 'hospital_death']
# names = std.columns
# rf=RandomForestClassifier()
# rf.fit(atopdata, atoptarget)
# atopimportance = rf.feature_importances_
# for k,l in enumerate(atopimportance):
#         print('Feature: %0d, Score: %.5f' % (k, l))
# pyplot.bar([x for x in range(len(atopimportance))], atopimportance)
# pyplot.show()
# print("Features sorted by their score:")
# print(sorted(zip(map(lambda x:round(x,4),rf.feature_importances_),names)))

## v0.5 remove attributs with importance<0.01
temp1 = [x for x in range(atop.shape[1])]
temp2 = [2,3,9,10,13,16,33,53]
temp3 = [x for x in range(65,95)]
tempf = []
for m in temp3:
       temp2.append(m)
# print(temp2)
for n in temp1:
       if n not in temp2:
              tempf.append(n)
#print(tempf)

atopdata = atop.iloc[:, tempf]
atoptarget = atop.loc[:, atop.columns == 'hospital_death']

atopdata

atoptarget

## v1.0 split train set and test set
features, targets = atopdata, atoptarget
train_features, test_features, train_targets, test_targets = train_test_split(features, targets,
                                                                              train_size=0.8,
                                                                              test_size=0.2,
                                                                              random_state=42,
                                                                              shuffle = True,
                                                                              stratify=targets
)

## v1.1 PCA
pca = PCA(0.95)
pca.fit(train_features)
# print(pca.n_components_) # = 32

train_features_pca = pca.transform(train_features)
test_features_pca = pca.transform(test_features)
all_features_pca = pca.transform(features)

## v1.2 DASK
def daskrun(MLmethod):
    daskop = Client(n_workers=4)
    daskop
    with joblib.parallel_backend('dask'):
        MLmethod.fit(train_features_pca, np.ravel(train_targets))
    print(MLmethod.score(test_features_pca, test_targets))



"""# Prediction

# KNN
"""

#Import knearest neighbors Classifier model
from sklearn.neighbors import KNeighborsClassifier

#Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=8)

#Train the model using the training sets
knn.fit(train_features_pca, np.ravel(train_targets))

#Predict the response for test dataset
y_pred = knn.predict(test_features_pca)

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(test_targets, y_pred))

# Check the model performance
print(classification_report(test_targets, y_pred))

"""# Gaussian Naive Bayes model"""

#Import Gaussian Naive Bayes model
from sklearn.naive_bayes import GaussianNB

#Create a Gaussian Classifier
gnb = GaussianNB()

#Train the model using the training sets
gnb.fit(train_features_pca, np.ravel(train_targets))

#Predict the response for test dataset
y_pred = gnb.predict(test_features_pca)

#Accuracy
print("Accuracy:", metrics.accuracy_score(test_targets, y_pred))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Precision:",metrics.precision_score(test_targets, y_pred))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Recall:",metrics.recall_score(test_targets, y_pred))

# Check the model performance
print(classification_report(test_targets, y_pred))

"""# DecisionTree"""

from sklearn.tree import DecisionTreeRegressor

# Create a decision tree regression model with default arguments
decision_tree = DecisionTreeRegressor()

# Fit the model to the training features and targets
decision_tree.fit(train_features_pca, np.ravel(train_targets))

#Predict the response for test dataset
y_pred = decision_tree.predict(test_features_pca)

#Accuracy
print("Accuracy:", metrics.accuracy_score(test_targets, y_pred))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Precision:",metrics.precision_score(test_targets, y_pred))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Recall:",metrics.recall_score(test_targets, y_pred))

# Check the model performance
print(classification_report(test_targets, y_pred))

"""# SVM"""

from sklearn import svm
from sklearn.metrics import recall_score
# Model and performance
from sklearn.svm import OneClassSVM
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

C = 1.0  # = self._alpha in our algorithm

model1 = svm.SVC(kernel='rbf', gamma=0.7, C=C)

# Train the one class support vector machine (SVM) model
one_class_svm = OneClassSVM(nu=0.01, kernel = 'rbf', gamma = 'auto').fit(train_features_pca)

# Predict the anomalies
prediction = one_class_svm.predict(test_features_pca)

# Change the anomalies' values to make it consistent with the true values
prediction = [1 if i==-1 else 0 for i in prediction]

# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(test_targets, prediction))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Precision:",metrics.precision_score(test_targets, prediction))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Recall:",metrics.recall_score(test_targets, prediction))

# Check the model performance
print(classification_report(test_targets, prediction))

"""# #Prediction with Oversampling

"""

## v1.3.2 oversampling
oversample = SMOTE(sampling_strategy=0.2, random_state=42)
train_features_pca_over, train_targets_over = oversample.fit_resample(train_features_pca, train_targets)
print(train_features_pca_over.shape)

"""# KNN"""

# Synthetic dataset
from sklearn.datasets import make_classification
# Model and performance
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

#Import knearest neighbors Classifier model
from sklearn.neighbors import KNeighborsClassifier

#Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=8)

#Train the model using the training sets
knn.fit(train_features_pca_over, np.ravel(train_targets_over))

#Predict the response for test dataset
y_pred = knn.predict(test_features_pca)

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(test_targets, y_pred))

# Check the model performance
print(classification_report(test_targets, y_pred))

"""# Gaussian Naive Bayes model"""

#Import Gaussian Naive Bayes model
from sklearn.naive_bayes import GaussianNB

#Create a Gaussian Classifier
gnb = GaussianNB()

#Train the model using the training sets
gnb.fit(train_features_pca_over, np.ravel(train_targets_over))

#Predict the response for test dataset
y_pred = gnb.predict(test_features_pca)

#Accuracy
print("Accuracy:", metrics.accuracy_score(test_targets, y_pred))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Precision:",metrics.precision_score(test_targets, y_pred))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Recall:",metrics.recall_score(test_targets, y_pred))

# Check the model performance
print(classification_report(test_targets, y_pred))

"""## DecisionTreeRegressor"""

from sklearn.tree import DecisionTreeRegressor

# Create a decision tree regression model with default arguments
decision_tree = DecisionTreeRegressor()

# Fit the model to the training features and targets
decision_tree.fit(train_features_pca_over, np.ravel(train_targets_over))

#Predict the response for test dataset
y_pred = decision_tree.predict(test_features_pca)

#Accuracy
print("Accuracy:", metrics.accuracy_score(test_targets, y_pred))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Precision:",metrics.precision_score(test_targets, y_pred))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Recall:",metrics.recall_score(test_targets, y_pred))

# Check the model performance
print(classification_report(test_targets, y_pred))

"""# SVM"""

from sklearn import svm
from sklearn.metrics import recall_score
# Model and performance
from sklearn.svm import OneClassSVM
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

C = 1.0  # = self._alpha in our algorithm

model1 = svm.SVC(kernel='rbf', gamma=0.7, C=C)

# Train the one class support vector machine (SVM) model
one_class_svm = OneClassSVM(nu=0.01, kernel = 'rbf', gamma = 'auto').fit(train_features_pca_over)

# Predict the anomalies
prediction = one_class_svm.predict(test_features_pca)

# Change the anomalies' values to make it consistent with the true values
prediction = [1 if i==-1 else 0 for i in prediction]

# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(test_targets, prediction))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Precision:",metrics.precision_score(test_targets, prediction))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Recall:",metrics.recall_score(test_targets, prediction))

# Check the model performance
print(classification_report(test_targets, prediction))

